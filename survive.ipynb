{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# active / inactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook generates acvtive / inactive table for fp shoplst daily.  \n",
    "there should be 2 functions, \n",
    "\n",
    "1. create active / inactive\n",
    "given a start and end date, this function should fetch all data in this time interval, concat them, \n",
    "and make an active table and inactive table.  \n",
    "\n",
    "2. update to target\n",
    "given a target date, this function should update the active table day by day until the target date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def deep_find_files(file_format, directory, *keywords):\n",
    "    '''\n",
    "    Use rglob to recursively search for files with the given format in the specified directory and all subdirectories\n",
    "    '''\n",
    "    return [\n",
    "        str(file)\n",
    "        for file in Path(directory).rglob(f'*.{file_format}')\n",
    "        if all(keyword in file.name for keyword in keywords)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new dates: 100%|██████████| 366/366 [00:02<00:00, 179.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1942 active shops to active_shops.csv\n",
      "Saved 142 inactive shops to inactive_shops.csv\n"
     ]
    }
   ],
   "source": [
    "def concat_folder(folder_path, max_workers=4):\n",
    "    \"\"\"\n",
    "    Read every CSV under `folder_path` (and subfolders) in parallel\n",
    "    and return one concatenated DataFrame (or empty DF if none).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        csv_files = deep_find_files('csv', folder_path)\n",
    "        if not csv_files:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        dfs = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            futures = {executor.submit(pd.read_csv, fp): fp for fp in csv_files}\n",
    "            for fut in futures:\n",
    "                try:\n",
    "                    dfs.append(fut.result())\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ failed to read {futures[fut]}: {e}\")\n",
    "\n",
    "        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Error in concat_folder({folder_path}):\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def update_survival_dfs(base_folder, start_date, end_date, max_workers=8,\n",
    "                        active_file=\"active_shops.csv\", \n",
    "                        inactive_file=\"inactive_shops.csv\"):\n",
    "    \"\"\"\n",
    "    Updates existing active/inactive shop files with new data.\n",
    "    \n",
    "    1) Reads existing active & inactive CSVs if they exist\n",
    "    2) Loads new data from specified date range\n",
    "    3) Combines all data and recalculates active/inactive status\n",
    "    4) Saves updated files\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Read existing files if they exist\n",
    "    existing_active = pd.DataFrame()\n",
    "    existing_inactive = pd.DataFrame()\n",
    "    \n",
    "    if os.path.exists(active_file):\n",
    "        try:\n",
    "            existing_active = pd.read_csv(active_file)\n",
    "            print(f\"Loaded {len(existing_active)} shops from {active_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {active_file}: {e}\")\n",
    "    \n",
    "    if os.path.exists(inactive_file):\n",
    "        try:\n",
    "            existing_inactive = pd.read_csv(inactive_file)\n",
    "            print(f\"Loaded {len(existing_inactive)} shops from {inactive_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {inactive_file}: {e}\")\n",
    "    \n",
    "    # Process new data\n",
    "    try:\n",
    "        # build list of date-folders\n",
    "        dates = pd.date_range(start=start_date, end=end_date)\n",
    "        folders = [f\"{base_folder}/{d.strftime('%Y-%m-%d')}\" for d in dates]\n",
    "        \n",
    "        # parallel load with progress bar\n",
    "        dfs = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for df in tqdm(\n",
    "                executor.map(lambda fp: concat_folder(fp, max_workers), folders),\n",
    "                total=len(folders),\n",
    "                desc=\"Processing new dates\",\n",
    "            ):\n",
    "                if not df.empty:\n",
    "                    dfs.append(df)\n",
    "        \n",
    "        if not dfs:\n",
    "            print(f\"No new data found in any folder from {start_date} to {end_date}\")\n",
    "            # If we have existing data, just return it\n",
    "            if not (existing_active.empty and existing_inactive.empty):\n",
    "                return existing_active, existing_inactive\n",
    "            else:\n",
    "                return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        # merge all daily data from new period\n",
    "        new_date_shoplst_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # ensure datetime\n",
    "        new_date_shoplst_df['scrape_batch_date'] = pd.to_datetime(\n",
    "            new_date_shoplst_df['scrape_batch_date'],\n",
    "            format='%Y-%m-%d'\n",
    "        )\n",
    "        \n",
    "        # Prepare existing data to merge with new data\n",
    "        existing_combined = pd.DataFrame()\n",
    "        if not (existing_active.empty and existing_inactive.empty):\n",
    "            # Convert date columns to datetime in existing data\n",
    "            for df in [existing_active, existing_inactive]:\n",
    "                if not df.empty:\n",
    "                    for col in ['scrape_batch_date', 'first_seen', 'last_seen']:\n",
    "                        if col in df.columns:\n",
    "                            df[col] = pd.to_datetime(df[col])\n",
    "            \n",
    "            # Combine existing active and inactive\n",
    "            existing_combined = pd.concat([existing_active, existing_inactive], ignore_index=True)\n",
    "            \n",
    "            # Extract just the raw data columns (excluding first_seen, last_seen)\n",
    "            cols_to_keep = [col for col in existing_combined.columns \n",
    "                           if col not in ['first_seen', 'last_seen']]\n",
    "            \n",
    "            existing_raw = existing_combined[cols_to_keep].copy()\n",
    "            \n",
    "            # Combine with new data\n",
    "            date_shoplst_df = pd.concat([existing_raw, new_date_shoplst_df], ignore_index=True)\n",
    "        else:\n",
    "            date_shoplst_df = new_date_shoplst_df\n",
    "            \n",
    "        # Remove potential duplicates (same shop on same day)\n",
    "        date_shoplst_df = date_shoplst_df.drop_duplicates(\n",
    "            subset=['shopCode', 'scrape_batch_date'], \n",
    "            keep='last'\n",
    "        )\n",
    "        \n",
    "        # compute first & last seen per shop\n",
    "        shop_dates = (\n",
    "            date_shoplst_df\n",
    "            .groupby('shopCode')['scrape_batch_date']\n",
    "            .agg(first_seen='min', last_seen='max')\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # grab each shop's full row at its last_seen date\n",
    "        last_record_df = (\n",
    "            date_shoplst_df\n",
    "            .sort_values(['shopCode','scrape_batch_date'], ascending=[True, False])\n",
    "            .drop_duplicates(subset='shopCode', keep='first')\n",
    "        )\n",
    "        \n",
    "        # merge to bring in first_seen/last_seen\n",
    "        merged = last_record_df.merge(shop_dates, on='shopCode', how='left')\n",
    "        \n",
    "        # reference = latest last_seen\n",
    "        reference_date = shop_dates['last_seen'].max()\n",
    "        merged['days_since_last'] = (reference_date - merged['last_seen']).dt.days\n",
    "        \n",
    "        # split\n",
    "        active_df = merged[merged['days_since_last'] < 60].drop(columns='days_since_last')\n",
    "        inactive_df = merged[merged['days_since_last'] >= 60].drop(columns='days_since_last')\n",
    "        \n",
    "        # Save updated files\n",
    "        active_df.to_csv(active_file, encoding=\"utf-8-sig\", index=False)\n",
    "        print(f\"Saved {len(active_df)} active shops to {active_file}\")\n",
    "        \n",
    "        inactive_df.to_csv(inactive_file, encoding=\"utf-8-sig\", index=False)\n",
    "        print(f\"Saved {len(inactive_df)} inactive shops to {inactive_file}\")\n",
    "        \n",
    "        return active_df, inactive_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error in update_survival_dfs:\")\n",
    "        traceback.print_exc()\n",
    "        if not (existing_active.empty and existing_inactive.empty):\n",
    "            print(\"Returning existing data without updates\")\n",
    "            return existing_active, existing_inactive\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "# usage - create new records\n",
    "active, inactive = update_survival_dfs(\n",
    "    base_folder=\"data/survive_test_data\",\n",
    "    start_date=\"2024-01-01\",  # Just process new dates\n",
    "    end_date=\"2024-12-31\",\n",
    "    max_workers=8,\n",
    "    active_file=\"active_shops.csv\",\n",
    "    inactive_file=\"inactive_shops.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1942 shops from active_shops.csv\n",
      "Loaded 142 shops from inactive_shops.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new dates: 100%|██████████| 1/1 [00:00<00:00, 316.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1942 active shops to active_shops.csv\n",
      "Saved 142 inactive shops to inactive_shops.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# usage - create new records\n",
    "active, inactive = update_survival_dfs(\n",
    "    base_folder=\"data/survive_test_data\",\n",
    "    start_date=\"2025-01-01\",  # Just process new dates\n",
    "    end_date=\"2025-01-01\",\n",
    "    max_workers=8,\n",
    "    active_file=\"active_shops.csv\",\n",
    "    inactive_file=\"inactive_shops.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selniumbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
