{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# active / inactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this notebook generates acvtive / inactive table for fp shoplst daily.  \n",
    "there should be 2 functions, \n",
    "\n",
    "1. create active / inactive\n",
    "given a start and end date, this function should fetch all data in this time interval, concat them, \n",
    "and make an active table and inactive table.  \n",
    "\n",
    "2. update to target\n",
    "given a target date, this function should update the active table day by day until the target date. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import traceback\n",
    "from typing import List, Optional, Dict\n",
    "\n",
    "def deep_find_files(file_format: str, directory: str, *keywords: str) -> List[str]:\n",
    "    '''\n",
    "    Use rglob to recursively search for files with the given format in the specified directory and all subdirectories\n",
    "    '''\n",
    "    return [\n",
    "        str(file)\n",
    "        for file in Path(directory).rglob(f'*.{file_format}')\n",
    "        if all(keyword in file.name for keyword in keywords)\n",
    "    ]\n",
    "\n",
    "def concat_folder(folder_path, keywords: List[str], max_workers=8):\n",
    "    \"\"\"\n",
    "    Read every CSV under `folder_path` (and subfolders) in parallel,\n",
    "    add a column `scrape_batch_date` extracted from the folder name,\n",
    "    and return one concatenated DataFrame (or empty DataFrame if none).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all CSV files in the folder (and its subfolders)\n",
    "        csv_files = deep_find_files('csv', folder_path, *keywords)\n",
    "        print(f\"Found {len(csv_files)} CSV files in {folder_path}\")\n",
    "        if not csv_files:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        # Assume folder_path ends with YYYY-MM-DD\n",
    "        date_str = os.path.basename(os.path.normpath(folder_path))\n",
    "\n",
    "        dfs = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit all reads in parallel\n",
    "            futures = {executor.submit(pd.read_csv, fp): fp for fp in csv_files}\n",
    "            for fut in futures:\n",
    "                fp = futures[fut]\n",
    "                try:\n",
    "                    df = fut.result()\n",
    "                    # Add the scrape_batch_date column\n",
    "                    df['scrape_batch_date'] = date_str\n",
    "                    dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ failed to read {fp}: {e}\")\n",
    "\n",
    "        # Concatenate or return empty DataFrame\n",
    "        return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "    except Exception:\n",
    "        print(f\"Error in concat_folder({folder_path}):\")\n",
    "        traceback.print_exc()\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_survival_dfs(base_folder, \n",
    "                        start_date, end_date,\n",
    "                        keywords,\n",
    "                        max_workers=8,\n",
    "                        active_file=\"active_shops.csv\", \n",
    "                        inactive_file=\"inactive_shops.csv\"):\n",
    "    \"\"\"\n",
    "    Updates existing active/inactive shop files with new data.\n",
    "    \n",
    "    1) Reads existing active & inactive CSVs if they exist\n",
    "    2) Loads new data from specified date range\n",
    "    3) Combines all data and recalculates active/inactive status\n",
    "    4) Saves updated files\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    # Read existing files if they exist\n",
    "    existing_active = pd.DataFrame()\n",
    "    existing_inactive = pd.DataFrame()\n",
    "    \n",
    "    if os.path.exists(active_file):\n",
    "        try:\n",
    "            existing_active = pd.read_csv(active_file)\n",
    "            print(f\"Loaded {len(existing_active)} shops from {active_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {active_file}: {e}\")\n",
    "    \n",
    "    if os.path.exists(inactive_file):\n",
    "        try:\n",
    "            existing_inactive = pd.read_csv(inactive_file)\n",
    "            print(f\"Loaded {len(existing_inactive)} shops from {inactive_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {inactive_file}: {e}\")\n",
    "    \n",
    "    # Process new data\n",
    "    try:\n",
    "        # build list of date-folders\n",
    "        dates = pd.date_range(start=start_date, end=end_date)\n",
    "        folders = [f\"{base_folder}/{d.strftime('%Y-%m-%d')}\" for d in dates]\n",
    "        \n",
    "        # parallel load with progress bar\n",
    "        dfs = []\n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            for df in tqdm(\n",
    "                executor.map(lambda fp: concat_folder(fp, keywords, max_workers), folders),\n",
    "                total=len(folders),\n",
    "                desc=\"Processing new dates\",\n",
    "            ):\n",
    "                if not df.empty:\n",
    "                    dfs.append(df)\n",
    "        \n",
    "        if not dfs:\n",
    "            print(f\"No new data found in any folder from {start_date} to {end_date}\")\n",
    "            # If we have existing data, just return it\n",
    "            if not (existing_active.empty and existing_inactive.empty):\n",
    "                return existing_active, existing_inactive\n",
    "            else:\n",
    "                return pd.DataFrame(), pd.DataFrame()\n",
    "        \n",
    "        # merge all daily data from new period\n",
    "        new_date_shoplst_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # ensure datetime\n",
    "        new_date_shoplst_df['scrape_batch_date'] = pd.to_datetime(\n",
    "            new_date_shoplst_df['scrape_batch_date'],\n",
    "            format='%Y-%m-%d'\n",
    "        )\n",
    "        \n",
    "        # Prepare existing data to merge with new data\n",
    "        existing_combined = pd.DataFrame()\n",
    "        if not (existing_active.empty and existing_inactive.empty):\n",
    "            # Convert date columns to datetime in existing data\n",
    "            for df in [existing_active, existing_inactive]:\n",
    "                if not df.empty:\n",
    "                    for col in ['scrape_batch_date', 'first_seen', 'last_seen']:\n",
    "                        if col in df.columns:\n",
    "                            df[col] = pd.to_datetime(df[col])\n",
    "            \n",
    "            # Combine existing active and inactive\n",
    "            existing_combined = pd.concat([existing_active, existing_inactive], ignore_index=True)\n",
    "            \n",
    "            # Extract just the raw data columns (excluding first_seen, last_seen)\n",
    "            cols_to_keep = [col for col in existing_combined.columns \n",
    "                           if col not in ['first_seen', 'last_seen']]\n",
    "            \n",
    "            existing_raw = existing_combined[cols_to_keep].copy()\n",
    "            \n",
    "            # Combine with new data\n",
    "            date_shoplst_df = pd.concat([existing_raw, new_date_shoplst_df], ignore_index=True)\n",
    "        else:\n",
    "            date_shoplst_df = new_date_shoplst_df\n",
    "            \n",
    "        # Remove potential duplicates (same shop on same day)\n",
    "        date_shoplst_df = date_shoplst_df.drop_duplicates(\n",
    "            subset=['shopCode', 'scrape_batch_date'], \n",
    "            keep='last'\n",
    "        )\n",
    "        \n",
    "        # compute first & last seen per shop\n",
    "        shop_dates = (\n",
    "            date_shoplst_df\n",
    "            .groupby('shopCode')['scrape_batch_date']\n",
    "            .agg(first_seen='min', last_seen='max')\n",
    "            .reset_index()\n",
    "        )\n",
    "        \n",
    "        # grab each shop's full row at its last_seen date\n",
    "        last_record_df = (\n",
    "            date_shoplst_df\n",
    "            .sort_values(['shopCode','scrape_batch_date'], ascending=[True, False])\n",
    "            .drop_duplicates(subset='shopCode', keep='first')\n",
    "        )\n",
    "        \n",
    "        # merge to bring in first_seen/last_seen\n",
    "        merged = last_record_df.merge(shop_dates, on='shopCode', how='left')\n",
    "        \n",
    "        # reference = latest last_seen\n",
    "        reference_date = shop_dates['last_seen'].max()\n",
    "        merged['days_since_last'] = (reference_date - merged['last_seen']).dt.days\n",
    "        \n",
    "        # split\n",
    "        active_df = merged[merged['days_since_last'] < 60].drop(columns='days_since_last')\n",
    "        inactive_df = merged[merged['days_since_last'] >= 60].drop(columns='days_since_last')\n",
    "        \n",
    "        # Save updated files\n",
    "        active_df.to_csv(active_file, encoding=\"utf-8-sig\", index=False)\n",
    "        print(f\"Saved {len(active_df)} active shops to {active_file}\")\n",
    "        \n",
    "        inactive_df.to_csv(inactive_file, encoding=\"utf-8-sig\", index=False)\n",
    "        print(f\"Saved {len(inactive_df)} inactive shops to {inactive_file}\")\n",
    "        \n",
    "        return active_df, inactive_df\n",
    "        \n",
    "    except Exception:\n",
    "        print(\"Error in update_survival_dfs:\")\n",
    "        traceback.print_exc()\n",
    "        if not (existing_active.empty and existing_inactive.empty):\n",
    "            print(\"Returning existing data without updates\")\n",
    "            return existing_active, existing_inactive\n",
    "        return pd.DataFrame(), pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implement on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new dates: 100%|██████████| 366/366 [00:01<00:00, 189.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1942 active shops to active_shops.csv\n",
      "Saved 142 inactive shops to inactive_shops.csv\n"
     ]
    }
   ],
   "source": [
    "# example usage - update test records\n",
    "# usage - create new records\n",
    "active, inactive = update_survival_dfs(\n",
    "    base_folder=\"data/survive_test_data\",\n",
    "    start_date=\"2024-01-01\",  # Just process new dates\n",
    "    end_date=\"2024-12-31\",\n",
    "    keywords=[],\n",
    "    max_workers=8,\n",
    "    active_file=\"active_shops.csv\",\n",
    "    inactive_file=\"inactive_shops.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new dates: 100%|██████████| 1/1 [00:00<00:00, 905.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new data found in any folder from 2025-01-01 to 2025-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# usage - create new records\n",
    "active, inactive = update_survival_dfs(\n",
    "    base_folder=\"data/survive_test_data\",\n",
    "    start_date=\"2025-01-01\",  # Just process new dates\n",
    "    end_date=\"2025-01-01\",\n",
    "    max_workers=8,\n",
    "    active_file=\"active_shops.csv\",\n",
    "    inactive_file=\"inactive_shops.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inplement on files in google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing new dates:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 543 CSV files in /Users/yun/Library/CloudStorage/GoogleDrive-racoffee33@gmail.com/.shortcut-targets-by-id/1ouHUfwV9gjb5MilbqRsuT55HCukAcivErdpeotDNOF0iimcyQ3_uQTwHwmAdJB_CMVcuQmNG/FoodpandaUbereat/foodpanda_crawler_beta_2/shopLst/2023-07-14\n",
      "Found 543 CSV files in /Users/yun/Library/CloudStorage/GoogleDrive-racoffee33@gmail.com/.shortcut-targets-by-id/1ouHUfwV9gjb5MilbqRsuT55HCukAcivErdpeotDNOF0iimcyQ3_uQTwHwmAdJB_CMVcuQmNG/FoodpandaUbereat/foodpanda_crawler_beta_2/shopLst/2023-07-15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bn/cmh08yxx3k7bh72tl2kp5drm0000gn/T/ipykernel_24493/3202663225.py:48: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
      "Processing new dates: 100%|██████████| 2/2 [00:04<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 59498 active shops to active_shops.csv\n",
      "Saved 0 inactive shops to inactive_shops.csv\n"
     ]
    }
   ],
   "source": [
    "# usage - create new records\n",
    "active, inactive = update_survival_dfs(\n",
    "    base_folder=\"/Users/yun/Library/CloudStorage/GoogleDrive-racoffee33@gmail.com/.shortcut-targets-by-id/1ouHUfwV9gjb5MilbqRsuT55HCukAcivErdpeotDNOF0iimcyQ3_uQTwHwmAdJB_CMVcuQmNG/FoodpandaUbereat/foodpanda_crawler_beta_2/shopLst\",\n",
    "    start_date=\"2023-07-14\",  # Just process new dates\n",
    "    end_date=\"2023-07-15\",\n",
    "    keywords=['shopLst'], \n",
    "    max_workers=8,\n",
    "    active_file=\"active_shops.csv\",\n",
    "    inactive_file=\"inactive_shops.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selniumbase",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
